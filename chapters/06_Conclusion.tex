\chapter{Conclusion}
The results of the test show that the single layered network is capable of navigating the robot to follow the ball. But also the limits of the simple structure is shown. Adding control over the head the DVS input lost its capabilities to be used as single point of information. As a centered target in the dvs input doesn't have to be in front of the robot any more, additional information has to be combined to distinguish the relative position of the target. This is related to the problem of linear separable data sets. Before the change spikes on one side of the DVS input could be directly translated to a control command. In this more complex setting none of the inputs on their own would allow to directly derive an action. For example the output of the had would be strong candidate to directly jump to an action, but if the robot head a bigger angle to target and starts reducing it, the head has to counteract this again to keep the ball in the center. This leads to a higher activity on the opposing neuron which on its own can not be distinguished from the higher activity coming from a movement to a ball at opposing direction. Only with the addition of information like the absolute angle of the head joint a valid decision can be made. This relation between inputs can only described by a more complex network. This would also explain why none of the single layer networks were able to stabilize for this configuration. More surprising was the bad performance of the network with one single layer for both outputs. Even though the weights seemed to have stabilised it wasn't able to control the robot. A possible reason might be that a shared hidden layer is more sensitive to weight changes above the hidden layer layer as these affect both neurons most likely in opposing direction amplifying the effect. Thus even longer training with smaller dopamine levels would be needed to lead to a stabilisation.
Regarding that it makes sense that the split network could learn faster as the weights above the hidden layer only adjusted for one neuron. The good results from this network are as astonishing as rewarding. But it should also be said that split hidden layer setup can be get from the single hidden layer network bey removing connections and with them the network loses complexity and theoretically gets weaker.
For the future another interesting idea would be to replace the part of the network responsible for controlling the head with for example another multilayer SNN trained for image recognition. Identifying different objects could be used to train the snake to evade or run from certain objects and be attracted to others. Being able to identify dangers as well as targets is a fundamental property for autonomous robots. 



% \begin{table}[htpb]
%   \caption[Parameters 2.Setup]{Parameters of body Neurons in the 2. setup} \label{tab:ParamsBase2N}
%   \begin{tabular}{|c| c |l|}
%       \toprule
%       Parameter & Value & Description \\
%       \midrule
%       $c_m$   & 20.0  & Capacity of the membrane \\
%       $tau_{m}$    & 50.0  & Membrane time constant \\
%       $tau_{refrac}$   & 1.  & Duration of refractory period\\
%       $v_{thresh}$   & -50.0  & Spike initiation threshold \\
%       $v_{reset}$    & -65.0  &  Reset value for $V_m$ after a spike \\
%       $v_{rest}$ & -65.0 & Resting voltage for $V_m$ \\
%       \bottomrule
%     \end{tabular}
%     \end{table}
%   \begin{table}[htpb]
%     \caption[Parameters 2.Setup]{Parameters of the body Synapses for the 2. setup} \label{tab:ParamsBase2S}
%     \begin{tabular}{|c| c |l|}
%         \toprule
%         Parameter  & Value & Description \\
%         \midrule
%         $W_{max}$ & 6000   & Maximum weight of synapse\\   
%         $W_{min}$ & -6000  & Minimum weight of synapse\\   
%         $A_{+}$   & 0.1    & Constant scaling strength of potentiation\\   
%         $A_{-}$   & -0.1   & Constant scaling strength of depression \\   
%         $\tau_c$  & 100.0   & Time constant of eligibility trace \\  
%         $\tau_n$  & 20.0   & Time constant of reward signal  \\   
%         $b$       & 0.0    & Baseline neuromodulator concentration \\    
%         \bottomrule
%     \end{tabular}
%     \end{table}


% \begin{figure}[htpb]
%   \centering
%   % This should probably go into a file in figures/
%   \begin{tikzpicture}[node distance=0.5cm]
%     \node (P0) {};
%     \node (P1) [right of=P0] {};
%     \node (P2) [right of=P1] {};
%     \node (P3) [right of=P2] {};
%     \node (P4) [right of=P3] {};
%     \node (P5) [right of=P4] {};
%     \node (P6) [right of=P5] {};
%     \node (P7) [right of=P6] {};
%     \node (P8) [right of=P7] {};
%     \node (P9) [right of=P8] {};

%     \node (I0) [below of=P0] {};
%     \node (I1) [right of=I0] {};
%     \node (I2) [right of=I1] {};
%     \node (I3) [right of=I2] {};
%     \node (I4) [right of=I3] {};
%     \node (I5) [right of=I4] {};
%     \node (I6) [right of=I5] {};
%     \node (I7) [right of=I6] {};
%     \node (I8) [right of=I7] {};
%     \node (I9) [right of=I8] {};

%     \node (O0) [below of=I0] {};
%     \node (O1) [right of=O0] {};

%     \node (D0) [below of=O0] {};
%     \node (D1) [below of=O1] {};

%     \path[every node]
%       (P0) edge (I0)
%       (P1) edge (I1)
%       (P2) edge (I2)
%       (P3) edge (I3)
%       (P4) edge (I4)
%       (P5) edge (I5)
%       (P6) edge (I6)
%       (P7) edge (I7)
%       (P8) edge (I8)
%       (P9) edge (I9)

%       (I0) edge (O0)
%       (I1) edge (O0)
%       (I2) edge (O0)
%       (I3) edge (O0)
%       (I4) edge (O0)
%       (I5) edge (O0)
%       (I6) edge (O0)
%       (I7) edge (O0)
%       (I8) edge (O0)
%       (I9) edge (O0)
      
%       (I0) edge (O1)
%       (I1) edge (O1)
%       (I2) edge (O1)
%       (I3) edge (O1)
%       (I4) edge (O1)
%       (I5) edge (O1)
%       (I6) edge (O1)
%       (I7) edge (O1)
%       (I8) edge (O1)
%       (I9) edge (O1)

%       (O0) edge (D0)
%       (O1) edge (D1);
%   \end{tikzpicture}
%   \caption[Simple Network]{Simple Network Topology}\label{fig:simpleNetwork}
% \end{figure}
